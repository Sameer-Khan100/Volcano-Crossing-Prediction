{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea48b27-4562-4248-b3d9-381ce4db456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class VolcanoProblem:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (3, 4)\n",
    "        self.start_state = (0, 0)\n",
    "        # self.end_states = {(2, 3), (1, 3)}\n",
    "        self.penalty_states = {(1, 1), (0, 2)}\n",
    "        self.penalty_value = -50\n",
    "        self.reward_states = {(2, 0), (0, 3)}\n",
    "        self.reward_values = {(2, 0): 20, (0, 3): 100}\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        # Randomly select a new initial state for each episode\n",
    "        available_states = list(set(self.grid_states()) - self.reward_states - self.penalty_states)\n",
    "        self.current_state = random.choice(available_states)\n",
    "        return self.current_state\n",
    "\n",
    "\n",
    "    def grid_states(self):\n",
    "        # Generate all possible states in the grid\n",
    "        rows, cols = self.grid_size\n",
    "        return [(i, j) for i in range(rows) for j in range(cols)]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.get_next_state(action)\n",
    "        reward = self.get_reward(next_state)\n",
    "        self.current_state = next_state  # Update the current state\n",
    "        return next_state, reward\n",
    "\n",
    "    def get_next_state(self, action):\n",
    "        current_row, current_col = self.current_state\n",
    "        if action == 'up' and current_row >= 1:\n",
    "            return current_row - 1, current_col\n",
    "        elif action == 'down' and current_row < self.grid_size[0] - 1:\n",
    "            return current_row + 1, current_col\n",
    "        elif action == 'left' and current_col >= 1:\n",
    "            return current_row, current_col - 1\n",
    "        elif action == 'right' and current_col < self.grid_size[1] - 1:\n",
    "            return current_row, current_col + 1\n",
    "        else:\n",
    "            return current_row, current_col\n",
    "\n",
    "\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state in self.penalty_states:\n",
    "            return self.penalty_value\n",
    "        elif state in self.reward_states:\n",
    "            return self.reward_values[state]\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b59ce-0ab2-42f8-aceb-78df4a1dbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epsilon_greedy_policy(Q_values, epsilon):\n",
    "#     if not Q_values:\n",
    "#         # If Q_values is empty, choose a random action\n",
    "#         return np.random.choice(['up', 'down', 'left', 'right'])\n",
    "#     if np.random.rand() < epsilon:\n",
    "#         # Explore: Choose a random action among available actions\n",
    "#         return np.random.choice(list(Q_values.keys()))\n",
    "#     else:\n",
    "#         # Exploit: Choose the action with the highest Q-value\n",
    "#         return max(Q_values, key=Q_values.get)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def model_free_monte_carlo(env, num_episodes=1000, epsilon=0.1, discount=1, alpha=0.1):\n",
    "    Q = {}\n",
    "    utilities = []\n",
    "    overall_utilities = []\n",
    "\n",
    "    all_states = env.penalty_states.union(env.reward_states)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        initial_state = state  # Store the initial state of the episode\n",
    "        episode_data = []\n",
    "\n",
    "        while state not in env.reward_states:\n",
    "            action = epsilon_greedy_policy(Q.get(tuple(state), {}), epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode_data.append((tuple(state), action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode_data))):\n",
    "            state, action, reward = episode_data[t]\n",
    "            G = discount * G + reward\n",
    "\n",
    "            current_q = Q.get(state, {}).get(action, 0)\n",
    "            Q.setdefault(state, {}).setdefault(action, current_q + alpha * (G - current_q))\n",
    "\n",
    "        state_tuple = tuple(initial_state)  # Use the initial state for printing and computing utilities\n",
    "        if state_tuple in Q:\n",
    "            avg_utility = np.mean(np.array(list(Q[state_tuple].values())))\n",
    "            utilities.append(avg_utility)\n",
    "        else:\n",
    "            utilities.append(0)\n",
    "        # Debugging statements\n",
    "        # print(f\"Episode: {episode}, Initial State: {state_tuple}, Q: {Q[state_tuple]}, Utility: {utilities[-1]}\")\n",
    "    overall_utilities.append(np.mean(utilities))\n",
    "    # print(f\"Overall Average Utility: {overall_utilities[0]}\")\n",
    "    return Q, overall_utilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon):\n",
    "    if Q and random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(list(Q.keys()))\n",
    "    elif Q:\n",
    "        return max(Q, key=Q.get)\n",
    "    else:\n",
    "        # If Q is empty, return a random action\n",
    "        return random.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "\n",
    "\n",
    "def sarsa(env, num_episodes=1000, epsilon=0.1, alpha=0.5, gamma=1):\n",
    "    Q = {}\n",
    "    overall_utilities = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        # Check if the environment has an 'actions' attribute\n",
    "        if hasattr(env, 'actions'):\n",
    "            possible_actions = env.actions\n",
    "        else:\n",
    "            # Use a default set of actions if 'actions' attribute is not present\n",
    "            possible_actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        action = epsilon_greedy_policy(Q.get(state, {}), epsilon)\n",
    "\n",
    "        while state not in env.reward_states and state not in env.penalty_states:\n",
    "            next_state, reward = env.step(action)\n",
    "            next_action = epsilon_greedy_policy(Q.get(next_state, {}), epsilon)\n",
    "\n",
    "            # Ensure keys are set for state-action pairs\n",
    "            Q.setdefault(state, {a: 0 for a in possible_actions})\n",
    "            Q.setdefault(next_state, {a: 0 for a in possible_actions})\n",
    "\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "            # Print debug statements\n",
    "            # print(f\"Episode: {episode}, State: {state}, Action: {action}, Next State: {next_state}, Next Action: {next_action}\")\n",
    "            # print(f\"Q[state]: {Q[state]}, Q[next_state]: {Q[next_state]}\")\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        # Calculate overall utility based on the average of maximum Q-values over all states\n",
    "        overall_utility = np.mean([max(actions.values()) for actions in Q.values()])\n",
    "        overall_utilities.append(overall_utility)\n",
    "    # Calculate the average overall utility over all episodes\n",
    "    average_overall_utility = np.mean(overall_utilities)\n",
    "    print(f\"Average Overall Utility: {average_overall_utility}\")\n",
    "    return Q, average_overall_utility\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(env, num_episodes=1000, epsilon=0.1, alpha=0.5, discount=1):\n",
    "    Q = {}\n",
    "    overall_utilities = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        # Check if the environment has an 'actions' attribute\n",
    "        if hasattr(env, 'actions'):\n",
    "            possible_actions = env.actions\n",
    "        else:\n",
    "            # Use a default set of actions if 'actions' attribute is not present\n",
    "            possible_actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        while state not in env.reward_states:\n",
    "            action = epsilon_greedy_policy(Q.get(state, {}), epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "\n",
    "            # Ensure keys are set for state-action pairs\n",
    "            Q.setdefault(state, {a: 0 for a in possible_actions})\n",
    "            Q.setdefault(next_state, {a: 0 for a in possible_actions})\n",
    "\n",
    "            best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "            Q[state][action] += alpha * (reward + discount * Q[next_state][best_next_action] - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Calculate overall utility based on the average of maximum Q-values over all states\n",
    "        overall_utility = np.mean([max(actions.values()) for actions in Q.values()])\n",
    "        overall_utilities.append(overall_utility)\n",
    "\n",
    "    # Calculate the average overall utility over all episodes\n",
    "    average_overall_utility = np.mean(overall_utilities)\n",
    "    print(f\"Average Overall Utility: {average_overall_utility}\")\n",
    "\n",
    "    return Q, average_overall_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524ccb9-8478-4ea1-893d-6e8f51f32008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e7be1-8c7c-41d5-837c-7503130d78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overall Utility: 40.106149855824405\n",
      "Average Overall Utility: 62.79747340961237\n",
      "Average Overall Utility: 41.61230465478507\n",
      "Average Overall Utility: 66.96884507556503\n",
      "Average Overall Utility: 27.888022689877292\n",
      "Average Overall Utility: 39.78667728640921\n",
      "Average Overall Utility: 38.83163358004542\n",
      "Average Overall Utility: 40.45384036829587\n",
      "Average Overall Utility: 65.817549129872\n",
      "Average Overall Utility: 40.052056753777144\n",
      "Average Overall Utility: 67.28165339328692\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "class VolcanoGUI:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Volcano Crossing Solver\")\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Slip Probability\n",
    "        self.slip_label = ttk.Label(self.master, text=\"Slip Probability:\")\n",
    "        self.slip_label.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "        self.slip_var = tk.DoubleVar()\n",
    "        self.slip_var.set(0.1)  # Initial slip probability\n",
    "\n",
    "        validate_cmd = self.master.register(self.validate_slip_entry)  # Register validation method\n",
    "\n",
    "        self.slip_entry = ttk.Entry(self.master, textvariable=self.slip_var, validate=\"key\", validatecommand=(validate_cmd, \"%P\"))\n",
    "        self.slip_entry.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "        # Epsilon\n",
    "        self.epsilon_label = ttk.Label(self.master, text=\"Epsilon Value:\")\n",
    "        self.epsilon_label.grid(row=1, column=0, padx=10, pady=10)\n",
    "\n",
    "        self.epsilon_var = tk.DoubleVar()\n",
    "        self.epsilon_var.set(0.2)  # Initial epsilon value\n",
    "        self.epsilon_entry = ttk.Entry(self.master, textvariable=self.epsilon_var)\n",
    "        self.epsilon_entry.grid(row=1, column=1, padx=10, pady=10)\n",
    "\n",
    "        # Number of Episodes\n",
    "        self.episodes_label = ttk.Label(self.master, text=\"Number of Episodes:\")\n",
    "        self.episodes_label.grid(row=2, column=0, padx=10, pady=10)\n",
    "\n",
    "        self.episodes_var = tk.IntVar()\n",
    "        self.episodes_var.set(1000)  # Initial number of episodes\n",
    "        self.episodes_entry = ttk.Entry(self.master, textvariable=self.episodes_var)\n",
    "        self.episodes_entry.grid(row=2, column=1, padx=10, pady=10)\n",
    "\n",
    "        # Run Button for Monte Carlo\n",
    "        self.run_monte_button = ttk.Button(self.master, text=\"Run Monte Carlo\", command=self.run_monte_carlo)\n",
    "        self.run_monte_button.grid(row=3, column=0, padx=10, pady=10)\n",
    "        \n",
    "        # Run Button for SARSA\n",
    "        self.run_sarsa_button = ttk.Button(self.master, text=\"Run SARSA\", command=self.run_sarsa)\n",
    "        self.run_sarsa_button.grid(row=3, column=1, padx=10, pady=10)\n",
    "\n",
    "        # Run Button for QLEARNING\n",
    "        self.run_qlearning_button = ttk.Button(self.master, text=\"Run Q LERANING\", command=self.run_qlearning)\n",
    "        self.run_qlearning_button.grid(row=3, column=2, padx=10, pady=10)\n",
    "\n",
    "         # Canvas for grid visualization\n",
    "        self.canvas = tk.Canvas(self.master, width=500, height=320, bg=\"white\")\n",
    "        self.canvas.grid(row=4, columnspan=3, padx=10, pady=10)\n",
    "\n",
    "\n",
    "    def validate_slip_entry(self, new_value):\n",
    "        try:\n",
    "            slip_value = float(new_value)\n",
    "            return 0.0 <= slip_value <= 0.3\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def draw_grid(self, env, Q_values=None):\n",
    "        cell_width = 100\n",
    "        cell_height = 80\n",
    "        top_margin = 20\n",
    "\n",
    "        # Calculate the total width and height of the grid area\n",
    "        total_width = env.grid_size[1] * cell_width\n",
    "        total_height = env.grid_size[0] * cell_height\n",
    "\n",
    "        # Calculate the margins to center the grid within the canvas\n",
    "        left_margin = (self.canvas.winfo_reqwidth() - total_width) // 2\n",
    "        top_margin = (self.canvas.winfo_reqheight() - total_height) // 2\n",
    "\n",
    "        for row in range(env.grid_size[0]):\n",
    "            for col in range(env.grid_size[1]):\n",
    "                x1, y1 = left_margin + col * cell_width, top_margin + row * cell_height\n",
    "                x2, y2 = x1 + cell_width, y1 + cell_height\n",
    "                state = (row, col)\n",
    "                color = \"white\"\n",
    "                if state in env.penalty_states:\n",
    "                    color = \"red\"\n",
    "                elif state in env.reward_states:\n",
    "                    color = \"green\"\n",
    "                self.canvas.create_rectangle(x1, y1, x2, y2, fill=color)\n",
    "\n",
    "                # Display States with Reward/Penalty values inside the box\n",
    "                if state in env.reward_values:\n",
    "                    reward = env.reward_values[state]\n",
    "                    self.canvas.create_text((x1 + x2) / 2, (y1 + y2) / 2, text=f\"Reward: {reward}\", fill=\"black\")\n",
    "                elif state in env.penalty_states:\n",
    "                    penalty_value = env.penalty_value\n",
    "                    self.canvas.create_text((x1 + x2) / 2, (y1 + y2) / 2, text=f\"Penalty: {penalty_value}\", fill=\"black\")\n",
    "                elif color == \"white\" and Q_values:\n",
    "                    # Display Q-values in white boxes\n",
    "                    q_values = Q_values.get(state, {})\n",
    "                    actions_text = \"\\n\".join([f\"{action}: {value:.2f}\" for action, value in q_values.items()])\n",
    "                    self.canvas.create_text((x1 + x2) / 2, (y1 + y2) / 2, text=actions_text, fill=\"black\")\n",
    "        \n",
    "    def run_sarsa(self):\n",
    "        epsilon_val = self.epsilon_var.get()\n",
    "        num_episodes = self.episodes_var.get()\n",
    "\n",
    "        env = VolcanoProblem()  # Make sure to replace this with the actual environment\n",
    "        Q_sarsa, overall_utility_sarsa = sarsa(env, num_episodes=num_episodes, epsilon=epsilon_val, alpha=0.9)\n",
    "\n",
    "        # Display the Q-values in a new window\n",
    "        result_window = tk.Toplevel(self.master)\n",
    "        result_window.title(\"SARSA Results\")\n",
    "\n",
    "        # Create a Treeview widget\n",
    "        q_values_tree = ttk.Treeview(result_window)\n",
    "        q_values_tree[\"columns\"] = (\"Action\", \"Q-value\")\n",
    "        q_values_tree.heading(\"#0\", text=\"State\")\n",
    "        q_values_tree.heading(\"Action\", text=\"Action\")\n",
    "        q_values_tree.heading(\"Q-value\", text=\"Q-value\")\n",
    "\n",
    "        # Insert Q-values into the Treeview\n",
    "        for state, actions in Q_sarsa.items():\n",
    "            state_str = f\"{state[0]}, {state[1]}\"  # Assuming state is a tuple (x, y)\n",
    "            for action, q_value in actions.items():\n",
    "                q_values_tree.insert(\"\", \"end\", text=state_str, values=(action, q_value))\n",
    "\n",
    "        q_values_tree.pack(pady=10)\n",
    "\n",
    "        # Display Overall Utility\n",
    "        overall_utility_label = ttk.Label(result_window, text=f\"Overall Average Utility (Sarsa): {overall_utility_sarsa}\")\n",
    "        overall_utility_label.pack(pady=10)\n",
    "\n",
    "        self.draw_grid(env, Q_sarsa)\n",
    "\n",
    "    def run_monte_carlo(self):\n",
    "        slip_prob = self.slip_var.get()\n",
    "        epsilon_val = self.epsilon_var.get()\n",
    "        num_episodes = self.episodes_var.get()\n",
    "\n",
    "        env = VolcanoProblem()\n",
    "        Q_monte_carlo, overall_utility = model_free_monte_carlo(env, num_episodes=num_episodes, epsilon=epsilon_val, discount=1)\n",
    "\n",
    "        # Display the Q-values in a new window\n",
    "        result_window = tk.Toplevel(self.master)\n",
    "        result_window.title(\"Monte Carlo Results\")\n",
    "\n",
    "        # Create a Treeview widget\n",
    "        q_values_tree = ttk.Treeview(result_window)\n",
    "        q_values_tree[\"columns\"] = (\"Action\", \"Q-value\")\n",
    "        q_values_tree.heading(\"#0\", text=\"State\")\n",
    "        q_values_tree.heading(\"Action\", text=\"Action\")\n",
    "        q_values_tree.heading(\"Q-value\", text=\"Q-value\")\n",
    "\n",
    "        # Insert Q-values into the Treeview\n",
    "        for state, actions in Q_monte_carlo.items():\n",
    "            state_str = f\"{state[0]}, {state[1]}\"  # Assuming state is a tuple (x, y)\n",
    "            for action, q_value in actions.items():\n",
    "                q_values_tree.insert(\"\", \"end\", text=state_str, values=(action, q_value))\n",
    "\n",
    "        q_values_tree.pack(pady=10)\n",
    "\n",
    "        # Display Overall Utility\n",
    "        overall_utility_label = ttk.Label(result_window, text=f\"Overall Average Utility (Monte Carlo): {overall_utility}\")\n",
    "        overall_utility_label.pack(pady=10)\n",
    "\n",
    "        self.draw_grid(env, Q_monte_carlo)\n",
    "\n",
    "    def run_qlearning(self):\n",
    "        epsilon_val = self.epsilon_var.get()\n",
    "        num_episodes = self.episodes_var.get()\n",
    "\n",
    "        env = VolcanoProblem()  # Make sure to replace this with the actual environment\n",
    "        Q_qlearning, overall_utility_qlearning = q_learning(env, num_episodes=num_episodes, epsilon=epsilon_val, alpha=0.5, discount=1)\n",
    "\n",
    "        # Display the Q-values in a new window\n",
    "        result_window = tk.Toplevel(self.master)\n",
    "        result_window.title(\"Q-learning Results\")\n",
    "\n",
    "        # Create a Treeview widget\n",
    "        q_values_tree = ttk.Treeview(result_window)\n",
    "        q_values_tree[\"columns\"] = (\"Action\", \"Q-value\")\n",
    "        q_values_tree.heading(\"#0\", text=\"State\")\n",
    "        q_values_tree.heading(\"Action\", text=\"Action\")\n",
    "        q_values_tree.heading(\"Q-value\", text=\"Q-value\")\n",
    "\n",
    "        # Insert Q-values into the Treeview\n",
    "        for state, actions in Q_qlearning.items():\n",
    "            state_str = f\"{state[0]}, {state[1]}\"  # Assuming state is a tuple (x, y)\n",
    "            for action, q_value in actions.items():\n",
    "                q_values_tree.insert(\"\", \"end\", text=state_str, values=(action, q_value))\n",
    "\n",
    "        q_values_tree.pack(pady=10)\n",
    "\n",
    "        # Display Overall Utility\n",
    "        overall_utility_label = ttk.Label(result_window, text=f\"Overall Average Utility (Q-Learning): {overall_utility_qlearning}\")\n",
    "        overall_utility_label.pack(pady=10)\n",
    "\n",
    "        self.draw_grid(env, Q_qlearning)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = VolcanoGUI(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66dc50-ea48-49a4-b64b-7c0db4ad5308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
